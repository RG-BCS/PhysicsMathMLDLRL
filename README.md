# Hi there, I’m PhysicsMathMLDLRL 

I’m a physicist turned self-taught Machine Learning engineer passionate about bridging theory and practice.

- Currently building and deploying projects across Traditional ML, Deep Learning, Reinforcement Learning, and Generative Models.
- I’ve built a strong foundation through self-study—starting with Learning Python to master the language itself—then diving into core texts like Hands-On Machine Learning, Deep Learning by Goodfellow, and a wide range of resources to go deep into machine learning, deep learning, and reinforcement learning.
- Projects include building a small GPT-2 style language model, several GAN variants (DCGAN, StyleGAN, CycleGAN), deep learning architectures such as CNNs, RNNs, LSTMs, GRUs, seq2seq models with and without attention for neural machine translation, Autoencoders and Variational Autoencoders (VAEs), customized implementations of traditional machine learning algorithms, and reinforcement learning agents applying algorithms like DQN and PPO.
- Focused on creating real-world applications grounded in solid mathematical and physical principles.

## My Self-Study Journey

As a self-taught ML engineer, I have studied a comprehensive range of foundational and advanced books, including:

- *Learning Python* by Mark Lutz  
- *Introduction to Machine Learning with Python* by Andreas C. Müller & Sarah Guido  
- *Machine Learning with Scikit-Learn and PyTorch* by Sebastian Raschka & Yuxi (Hayden) Liu  
- *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* by Aurélien Géron  
- *Deep Learning* by Ian Goodfellow, Yoshua Bengio, and Aaron Courville  
- *Deep Learning with Python* by François Chollet  
- *Advanced Deep Learning with Python* by Ivan Vasilev, Daniel Slater, Gianmario Spacagna, Peter Roelants, and Valentino Zocca  
- *Designing Machine Learning Systems* by Chip Huyen  
- *Pattern Recognition and Machine Learning* by Christopher M. Bishop  
- *Building Large Language Models* by Sebastian Raschka (2025)  

This rigorous self-study has equipped me to build projects spanning traditional ML, deep learning architectures including transformers, CNNs, RNNs, LSTMs, GRUs, GANs, VAEs, reinforcement learning, and language models.

---

## Technical Strengths

- **Languages**: Python (NumPy, Pandas, Matplotlib, etc.)
- **Frameworks**: PyTorch, TensorFlow, Keras, Scikit-learn, OpenAI Gym
- **Core Areas**:
  - Supervised / Unsupervised ML
  - Deep Learning Architectures (CNNs, RNNs, Transformers, GPT)
  - Autoencoders, VAEs, GANs
  - Reinforcement Learning (Q-learning, DQN, PPO, Actor-Critic)
  - Natural Language Processing (Seq2Seq, Attention, Transformers)
---

##  Selected Project Categories

Below is a curated overview of my key GitHub repositories:

---

###  Custom ML Algorithms

**Repo**: `custom-ml-algorithms/`  
From-scratch implementations of core ML algorithms using NumPy:

- Logistic Regression (binary & multiclass) w/ regularization & early stopping  
- Linear Regression w/ L1/L2, full-batch GD  
- K-Nearest Neighbors (KNN), Gaussian Naive Bayes  
- PCA (dimensionality reduction), Perceptron, Adaline  
- Ensemble models: Bagging, Voting Classifier  
- Custom K-Fold Cross Validation

---

### Reinforcement Learning  
**Repo**: `reinforcement-learning-algorithms/`  

- Vanilla DQN, Double DQN, Dueling DQN, PER  
- Policy Gradient (REINFORCE) w/ and w/o baseline  
- Actor-Critic, Tabular Q-Learning, TD(0)  
- NxN Gridworld MDPs: deterministic & stochastic  
- Value Iteration, Q-value Iteration, Visualization tools  
- TensorFlow & PyTorch implementations

---

### MLP Architectures  
**Repo**: `mlp_basic/`

- MLP from scratch vs PyTorch on MNIST  
- Heart Disease prediction using PyTorch  
- TensorFlow/Keras examples using Sequential, Functional, and Subclassing APIs

---

### Convolutional Neural Networks  
**Repo**: `cnvolutional-neural-networks/`

- LeNet-5, AlexNet, GoogLeNet (Inception v1)  
- Custom ResNet & Pre-activation ResNet from scratch  
- ResNet34: Fine-tuning vs feature extraction  
- CNN vs MLP on Fashion-MNIST

---

### Recurrent Neural Networks  
**Repo**: `rnn_models/`

- GRU: Custom-built, uni-directional, bi-directional  
- LSTM: Built from scratch for binary counting & sine wave prediction  
- Character-level LSTM trained on Jules Verne’s *The Mysterious Island*

---

### Seq2Seq + Attention  
**Repo**: `seq2seq-models/`

- Seq2Seq NMT models (English→French)  
- Luong-style attention (dot, general, concat)  
- Bahdanau attention (additive)  
- Built using bidirectional GRUs

---

### Transformer Models  
**Repo**: `transformer-models/`

- Full Transformer Encoder from scratch (multi-head attention, masking, FFN)  
- GPT-style decoder models (tiny stories generation)  
- English→Spanish translation using Transformer  
- Emotion & sentiment classification using custom Transformer encoders  
- No use of HuggingFace APIs — all built manually using PyTorch

---

### Generative Models (GANs)  
**Repo**: `gan-models/`

- CGANs (MNIST, Fashion-MNIST)  
- DCGAN, CycleGAN, Pix2Pix (CMP Facade)  
- WGAN-GP for single-image super-resolution (DIV2K)  
- MLP-based GANs vs CNN-GANs  
- Label-conditioned generation + class control

---

### Autoencoders  
**Repo**: `autoencoder-models/`

- Convolutional autoencoders for denoising Fashion-MNIST  
- PCA vs Autoencoder for dimensionality reduction  
- KMeans + t-SNE visualizations for latent space

---

### Variational Autoencoders (VAEs)  
**Repo**: `vae-models/`

- Beta-VAE on MNIST for latent disentanglement  
- VAE on UTKFace: face morphing, interpolation  
- Anomaly detection using VAE trained on normal digits only  
- KL annealing, cosine LR decay, and reconstruction loss-based detection

---

## Philosophy

I build projects with an emphasis on:

- Solid mathematical foundations  
- End-to-end reproducibility  
- Modular and readable code  
- Benchmarking against industry-standard implementations  
- Clarity over complexity — no black boxes
  
---

## How to reach me
Feel free to connect email me at nattyhena@yahoo.com 

---

*Thanks for visiting my profile! Feel free to explore my projects below.*

